{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Data‑Jobs Salary & Cost‑of‑Living Analysis with PySpark + Kafka\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from confluent_kafka import Producer\n",
    "from pyspark.sql import SparkSession, functions as F, types as T"
   ],
   "id": "230f0e460d43ad48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 1. Spark & Kafka setup\n",
    "# ---------------------------------------------------------------------------\n",
    "KAFKA_BOOTSTRAP_SERVERS = \"localhost:9092\"\n",
    "TOPIC_MEDIAN_SALARY = \"median_salary\"\n",
    "TOPIC_JOBS_PER_CAPITA = \"jobs_per_capita\"\n",
    "TOPIC_SCORE_RANKING = \"score_ranking\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Data‑Jobs‑PySpark‑App\")\n",
    "    .master(\"spark://172.29.16.102:7077\")  # run on cluster\n",
    "    .config(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "producer = Producer({\"bootstrap.servers\": KAFKA_BOOTSTRAP_SERVERS})\n",
    "\n",
    "def _delivery_report(err, msg):\n",
    "    if err:\n",
    "        print(f\"Kafka delivery failed: {err}\")\n",
    "    else:\n",
    "        print(f\"Message delivered to {msg.topic()} partition {msg.partition()} offset {msg.offset()}\")"
   ],
   "id": "525d60c97341fcc7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 2. Read input data (CSV → Spark DataFrame)\n",
    "# ---------------------------------------------------------------------------\n",
    "root = Path(\"../output\")\n",
    "jobs_path = (root / \"ScrapedJobOffersAllCleaned.csv\").as_posix()\n",
    "cost_path = (root / \"CostOfLivingAPI.csv\").as_posix()\n",
    "\n",
    "jobs = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .option(\"encoding\", \"utf-8\")\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(jobs_path)\n",
    ")\n",
    "\n",
    "cost_of_living = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(cost_path)\n",
    ")"
   ],
   "id": "4640b6b489b2fb44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 3. Helper columns & UDFs\n",
    "# ---------------------------------------------------------------------------\n",
    "@F.udf(returnType=T.StringType())\n",
    "def classify_role(title: str | None) -> str | None:\n",
    "    if not title:\n",
    "        return None\n",
    "    t = title.lower()\n",
    "    if \"scientist\" in t:\n",
    "        return \"Scientist\"\n",
    "    if \"engineer\" in t:\n",
    "        return \"Engineer\"\n",
    "    if \"analyst\" in t:\n",
    "        return \"Analyst\"\n",
    "    return None\n",
    "\n",
    "@F.udf(returnType=T.StringType())\n",
    "def extract_data_role(title: str | None) -> str | None:\n",
    "    if not title:\n",
    "        return None\n",
    "    t = title.lower()\n",
    "    if \"analyst\" in t:\n",
    "        return \"Data Analyst\"\n",
    "    if \"engineer\" in t:\n",
    "        return \"Data Engineer\"\n",
    "    if \"scientist\" in t:\n",
    "        return \"Data Scientist\"\n",
    "    return None\n",
    "\n",
    "jobs = jobs.withColumn(\"role\", classify_role(\"job_title\")).withColumn(\n",
    "    \"data_role\", extract_data_role(\"job_title\")\n",
    ")\n",
    "\n",
    "# Netto‑Umrechnung (simple 62% for AT & DE)\n",
    "jobs = jobs.withColumn(\n",
    "    \"annual_salary_netto\",\n",
    "    F.when(F.lower(\"country\").isin(\"austria\", \"germany\"), F.col(\"annual_salary\") * 0.62),\n",
    ")\n",
    "\n",
    "# Harmonise names\n",
    "for col_name in (\"city\", \"country\"):\n",
    "    jobs = jobs.withColumn(col_name, F.initcap(F.trim(col_name)))\n",
    "    cost_of_living = cost_of_living.withColumn(col_name, F.initcap(F.trim(col_name)))"
   ],
   "id": "1789b7c597cb2a19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 4. Median salary per city/role (Spark) → Kafka + Plot\n",
    "# ---------------------------------------------------------------------------\n",
    "median_salary_city = (\n",
    "    jobs.filter(\"role IS NOT NULL\")\n",
    "    .groupBy(\"city\")\n",
    "    .agg(F.expr(\"percentile_approx(annual_salary, 0.5)\").alias(\"median_salary\"))\n",
    "    .orderBy(F.desc(\"median_salary\"))\n",
    ")\n",
    "\n",
    "median_salary_role = (\n",
    "    jobs.filter(\"role IS NOT NULL\")\n",
    "    .groupBy(\"role\")\n",
    "    .agg(F.expr(\"percentile_approx(annual_salary, 0.5)\").alias(\"median_salary\"))\n",
    "    .orderBy(F.desc(\"median_salary\"))\n",
    ")\n",
    "\n",
    "# ---------- publish to Kafka\n",
    "for row in median_salary_city.toJSON().collect():\n",
    "    producer.produce(TOPIC_MEDIAN_SALARY, value=row, callback=_delivery_report)\n",
    "producer.flush()\n",
    "\n",
    "# ---------- plot on driver\n",
    "median_city_pd = median_salary_city.toPandas()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([\n",
    "    jobs.filter(F.col(\"city\") == city).select(\"annual_salary\").toPandas()[\"annual_salary\"]\n",
    "    for city in median_city_pd[\"city\"]\n",
    "], labels=median_city_pd[\"city\"], showfliers=True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Gehaltsverteilung nach Stadt\")\n",
    "plt.ylabel(\"Jahresgehalt (€)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "147d8303adf06c45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 5. Jobs per 100k inhabitants\n",
    "# ---------------------------------------------------------------------------\n",
    "EINWOHNER: Dict[str, int] = {\n",
    "    \"Berlin\": 3_782_202,\n",
    "    \"Hamburg\": 1_819_160,\n",
    "    \"München\": 1_510_378,\n",
    "    \"Frankfurt\": 775_790,\n",
    "    \"Köln\": 1_087_353,\n",
    "    \"Wien\": 2_028_399,\n",
    "    \"Linz\": 213_574,\n",
    "    \"Salzburg\": 157_652,\n",
    "    \"Graz\": 305_232,\n",
    "}\n",
    "\n",
    "pop_df = spark.createDataFrame([(k, v) for k, v in EINWOHNER.items()], [\"city\", \"einwohner\"])\n",
    "\n",
    "job_counts = (\n",
    "    jobs.filter(\"data_role IS NOT NULL\")\n",
    "    .groupBy(\"data_role\", \"city\")\n",
    "    .count()\n",
    "    .join(pop_df, \"city\", \"inner\")\n",
    "    .withColumn(\"jobs_pro_100k\", F.col(\"count\") / F.col(\"einwohner\") * 100_000)\n",
    ")\n",
    "\n",
    "for row in job_counts.toJSON().collect():\n",
    "    producer.produce(TOPIC_JOBS_PER_CAPITA, value=row, callback=_delivery_report)\n",
    "producer.flush()\n",
    "\n",
    "# Plot (driver)\n",
    "job_counts_pd = job_counts.orderBy(F.desc(\"jobs_pro_100k\")).toPandas()\n",
    "\n",
    "plt.figure(figsize=(14, max(6, len(job_counts_pd) * 0.3)))\n",
    "plt.barh(\n",
    "    job_counts_pd[\"data_role\"] + \" – \" + job_counts_pd[\"city\"],\n",
    "    job_counts_pd[\"jobs_pro_100k\"],\n",
    ")\n",
    "plt.xlabel(\"Jobangebote pro 100.000 Einwohner\")\n",
    "plt.title(\"Relative Anzahl der Data‑Jobangebote pro Stadt\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "68a4c48abd26b45d"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 6. Cost‑of‑living vs Data‑job salary score\n",
    "# ---------------------------------------------------------------------------\n",
    "# 6.1 Prepare cost‑of‑living totals\n",
    "cost_of_living = cost_of_living.withColumn(\n",
    "    \"total_costs\",\n",
    "    F.col(\"apt1_city_centre\") + F.col(\"ticket_monthly\") + F.col(\"utilities\") + F.col(\"internet\"),\n",
    ")\n",
    "\n",
    "cost_of_living = cost_of_living.withColumn(\n",
    "    \"score_allgemein\", F.col(\"net_salary\") / F.col(\"total_costs\")\n",
    ")\n",
    "\n",
    "# 6.2 Average netto data‑job salary per city (Spark)\n",
    "avg_netto = (\n",
    "    jobs.groupBy(\"city\", \"country\")\n",
    "    .agg(F.avg(\"annual_salary_netto\").alias(\"avg_datajob_netto\"))\n",
    ")\n",
    "\n",
    "# 6.3 Merge + derive scores\n",
    "merged = (\n",
    "    cost_of_living.join(avg_netto, [\"city\", \"country\"], \"inner\")\n",
    "    .withColumn(\"monthly_datajob_netto\", F.col(\"avg_datajob_netto\") / 12)\n",
    "    .withColumn(\"score_datajob\", F.col(\"monthly_datajob_netto\") / F.col(\"total_costs\"))\n",
    "    .withColumn(\"score_diff\", F.col(\"score_datajob\") - F.col(\"score_allgemein\"))\n",
    ")\n",
    "\n",
    "# 6.4 Ranking & Kafka\n",
    "ranking_cols = [\n",
    "    \"city\",\n",
    "    \"country\",\n",
    "    \"net_salary\",\n",
    "    \"monthly_datajob_netto\",\n",
    "    \"total_costs\",\n",
    "    \"score_allgemein\",\n",
    "    \"score_datajob\",\n",
    "    \"score_diff\",\n",
    "]\n",
    "ranking = merged.select(ranking_cols).orderBy(F.desc(\"score_datajob\"))\n",
    "\n",
    "for row in ranking.limit(50).toJSON().collect():  # top 50 entries\n",
    "    producer.produce(TOPIC_SCORE_RANKING, value=row, callback=_delivery_report)\n",
    "producer.flush()\n",
    "\n",
    "ranking_pd = ranking.orderBy(F.desc(\"score_diff\")).limit(8).toPandas()\n",
    "\n",
    "# 6.5 Plot score difference\n",
    "colors = [\"#2ECC71\" if val >= 0 else \"#E74C3C\" for val in ranking_pd[\"score_diff\"]]\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.barh(ranking_pd[\"city\"], ranking_pd[\"score_diff\"], color=colors)\n",
    "plt.xlabel(\"Score‑Differenz (Data‑Job – Allgemein)\")\n",
    "plt.title(\"Vorteil durch Data‑Job‑Einkommen im Vergleich zum Durchschnitt\")\n",
    "plt.axvline(0, color=\"gray\", linewidth=1)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 7. Graceful shutdown\n",
    "# ---------------------------------------------------------------------------\n",
    "producer.flush()\n",
    "spark.stop()\n",
    "\n",
    "print(\"Analysis finished. Aggregates published to Kafka.\")"
   ],
   "id": "d6d9a57257b11663"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
